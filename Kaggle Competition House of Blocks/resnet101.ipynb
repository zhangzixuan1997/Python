{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\n\nPATH = \"/kaggle/input/applications-of-deep-learning-wustl-fall-2020/final-kaggle-data/\"\nPATH_TRAIN = os.path.join(PATH, \"train.csv\")\nPATH_TEST = os.path.join(PATH, \"test.csv\")","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train = pd.read_csv(PATH_TRAIN)\ndf_test = pd.read_csv(PATH_TEST)\n\ndf_train = df_train[df_train.id != 1300]\n\ndf_train['filename'] = df_train[\"id\"].astype(str)+\".png\"\ndf_train['stable'] = df_train['stable'].astype(str)\n\ndf_test['filename'] = df_test[\"id\"].astype(str)+\".png\"","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TEST_PCT = 0.2\nTEST_CUT = int(len(df_train) * TEST_PCT)\n\ndf_train_cut = df_train[TEST_CUT:]\ndf_validate_cut = df_train[0:TEST_CUT]\n\nprint(f\"Training size: {len(df_train_cut)}\")\nprint(f\"Validate size: {len(df_validate_cut)}\")","execution_count":3,"outputs":[{"output_type":"stream","text":"Training size: 32785\nValidate size: 8196\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nimport keras_preprocessing\nfrom keras_preprocessing import image\nfrom keras_preprocessing.image import ImageDataGenerator\n\nWIDTH = 640\nHEIGHT = 400\ntraining_datagen = ImageDataGenerator(\n  rescale = 1./255,\n  horizontal_flip=True,\n#   featurewise_center = True,\n#   zca_epsilon = 0.001,\n#   zca_whitening = True,\n#   zoom_range = [0.7,0.8],\n  brightness_range = [1.2,1.6],\n  #Original set it to True, I will set False\n  vertical_flip = False,\n  fill_mode='nearest')\n\ntrain_generator = training_datagen.flow_from_dataframe(\n        dataframe=df_train_cut,\n        directory=PATH,\n        x_col=\"filename\",\n        y_col=\"stable\",\n        target_size=(HEIGHT, WIDTH),\n        batch_size=8,\n        class_mode='binary')\n\nvalidation_datagen = ImageDataGenerator(rescale = 1./255,\n#                                           featurewise_center = True,\n#                                           zca_epsilon = 0.001,\n#                                           zca_whitening = True,\n#                                           zoom_range = [0.7,0.8],\n                                          brightness_range = [1.2,1.5])\n\nval_generator = validation_datagen.flow_from_dataframe(\n        dataframe=df_validate_cut,\n        directory=PATH,\n        x_col=\"filename\",\n        y_col=\"stable\",\n        target_size=(HEIGHT, WIDTH),\n        class_mode='binary')","execution_count":4,"outputs":[{"output_type":"stream","text":"Found 32785 validated image filenames belonging to 2 classes.\nFound 8196 validated image filenames belonging to 2 classes.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Learning Rate Schedule\ndef lr_schedule(epoch):\n    lr = 0.0001\n    if epoch > 20:\n        lr = 0.000001\n    elif epoch > 15:\n        lr = 0.00001\n    elif epoch > 10:\n        lr = 0.00006\n    elif epoch > 5:\n        lr = 0.0001\n    print('Learning rate: ', lr)\n    return lr\nlr_callback = tf.keras.callbacks.LearningRateScheduler(lr_schedule, verbose=True)\n\nlr_callback = tf.keras.callbacks.LearningRateScheduler(lr_schedule, verbose=True)\n\n# #Learning Rate Annealer\n# from keras.callbacks import ReduceLROnPlateau\n# lrr= ReduceLROnPlateau(monitor='val_loss',   factor=.01,   patience=1,  min_lr=1e-6)","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers import Conv2D, Dense, Dropout, Flatten,BatchNormalization, GlobalAveragePooling2D\nfrom tensorflow.keras.optimizers import RMSprop\nfrom keras.applications import *\nfrom keras.models import Model\nimport keras\n\n# base_model = tf.keras.applications.Xception(input_shape=(HEIGHT,WIDTH, 3), include_top=False)\n# base_model.trainable = True\n\n# base_model = ResNet50(include_top=False, input_shape=(HEIGHT,WIDTH, 3))\n# base_model.trainable = True\n\n\nmonitor = EarlyStopping(monitor='val_loss', min_delta=1e-4, patience=8, verbose=1, mode='auto',\nrestore_best_weights=True)\nmodel = Sequential()\nbase_model = ResNet101(include_top=False, input_shape=(HEIGHT,WIDTH, 3))\nbase_model.trainable = True\nmodel.add(base_model)\nmodel.add(GlobalAveragePooling2D())\nmodel.add(Flatten())\nmodel.add(Dense(1, activation='sigmoid'))\ncallback_list = [monitor, lr_callback]\nmodel.compile(loss = 'binary_crossentropy', optimizer='adam')","execution_count":6,"outputs":[{"output_type":"stream","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet101_weights_tf_dim_ordering_tf_kernels_notop.h5\n171450368/171446536 [==============================] - 2s 0us/step\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":7,"outputs":[{"output_type":"stream","text":"Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nresnet101 (Functional)       (None, 13, 20, 2048)      42658176  \n_________________________________________________________________\nglobal_average_pooling2d (Gl (None, 2048)              0         \n_________________________________________________________________\nflatten (Flatten)            (None, 2048)              0         \n_________________________________________________________________\ndense (Dense)                (None, 1)                 2049      \n=================================================================\nTotal params: 42,660,225\nTrainable params: 42,554,881\nNon-trainable params: 105,344\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(train_generator,  \n  verbose = 1, \n  validation_data=val_generator, \n  steps_per_epoch=500, \n  validation_steps=200,\n  callbacks=callback_list, \n  epochs=25)","execution_count":11,"outputs":[{"output_type":"stream","text":"Learning rate:  0.0001\n\nEpoch 00001: LearningRateScheduler reducing learning rate to 0.0001.\nEpoch 1/25\n500/500 [==============================] - 402s 804ms/step - loss: 0.6033 - val_loss: 0.7233\nLearning rate:  0.0001\n\nEpoch 00002: LearningRateScheduler reducing learning rate to 0.0001.\nEpoch 2/25\n500/500 [==============================] - 383s 766ms/step - loss: 0.5024 - val_loss: 0.2941\nLearning rate:  0.0001\n\nEpoch 00003: LearningRateScheduler reducing learning rate to 0.0001.\nEpoch 3/25\n500/500 [==============================] - 379s 758ms/step - loss: 0.2181 - val_loss: 0.1209\nLearning rate:  0.0001\n\nEpoch 00004: LearningRateScheduler reducing learning rate to 0.0001.\nEpoch 4/25\n500/500 [==============================] - 379s 758ms/step - loss: 0.1406 - val_loss: 0.1218\nLearning rate:  0.0001\n\nEpoch 00005: LearningRateScheduler reducing learning rate to 0.0001.\nEpoch 5/25\n500/500 [==============================] - 379s 758ms/step - loss: 0.1229 - val_loss: 0.0917\nLearning rate:  0.0001\n\nEpoch 00006: LearningRateScheduler reducing learning rate to 0.0001.\nEpoch 6/25\n500/500 [==============================] - 379s 757ms/step - loss: 0.0921 - val_loss: 0.2246\nLearning rate:  0.0001\n\nEpoch 00007: LearningRateScheduler reducing learning rate to 0.0001.\nEpoch 7/25\n500/500 [==============================] - 378s 757ms/step - loss: 0.0851 - val_loss: 0.0697\nLearning rate:  0.0001\n\nEpoch 00008: LearningRateScheduler reducing learning rate to 0.0001.\nEpoch 8/25\n500/500 [==============================] - 378s 757ms/step - loss: 0.0627 - val_loss: 0.1184\nLearning rate:  0.0001\n\nEpoch 00009: LearningRateScheduler reducing learning rate to 0.0001.\nEpoch 9/25\n500/500 [==============================] - 379s 758ms/step - loss: 0.0434 - val_loss: 0.0388\nLearning rate:  0.0001\n\nEpoch 00010: LearningRateScheduler reducing learning rate to 0.0001.\nEpoch 10/25\n500/500 [==============================] - 378s 756ms/step - loss: 0.0664 - val_loss: 0.0408\nLearning rate:  0.0001\n\nEpoch 00011: LearningRateScheduler reducing learning rate to 0.0001.\nEpoch 11/25\n500/500 [==============================] - 378s 757ms/step - loss: 0.0308 - val_loss: 0.0357\nLearning rate:  6e-05\n\nEpoch 00012: LearningRateScheduler reducing learning rate to 6e-05.\nEpoch 12/25\n500/500 [==============================] - 377s 754ms/step - loss: 0.0359 - val_loss: 0.0606\nLearning rate:  6e-05\n\nEpoch 00013: LearningRateScheduler reducing learning rate to 6e-05.\nEpoch 13/25\n500/500 [==============================] - 379s 758ms/step - loss: 0.0233 - val_loss: 0.0330\nLearning rate:  6e-05\n\nEpoch 00014: LearningRateScheduler reducing learning rate to 6e-05.\nEpoch 14/25\n500/500 [==============================] - 377s 755ms/step - loss: 0.0354 - val_loss: 0.0244\nLearning rate:  6e-05\n\nEpoch 00015: LearningRateScheduler reducing learning rate to 6e-05.\nEpoch 15/25\n500/500 [==============================] - 377s 754ms/step - loss: 0.0239 - val_loss: 0.0345\nLearning rate:  6e-05\n\nEpoch 00016: LearningRateScheduler reducing learning rate to 6e-05.\nEpoch 16/25\n500/500 [==============================] - 378s 756ms/step - loss: 0.0256 - val_loss: 0.0212\nLearning rate:  1e-05\n\nEpoch 00017: LearningRateScheduler reducing learning rate to 1e-05.\nEpoch 17/25\n500/500 [==============================] - 379s 758ms/step - loss: 0.0120 - val_loss: 0.0145\nLearning rate:  1e-05\n\nEpoch 00018: LearningRateScheduler reducing learning rate to 1e-05.\nEpoch 18/25\n500/500 [==============================] - 379s 759ms/step - loss: 0.0136 - val_loss: 0.0144\nLearning rate:  1e-05\n\nEpoch 00019: LearningRateScheduler reducing learning rate to 1e-05.\nEpoch 19/25\n500/500 [==============================] - 379s 759ms/step - loss: 0.0094 - val_loss: 0.0086\nLearning rate:  1e-05\n\nEpoch 00020: LearningRateScheduler reducing learning rate to 1e-05.\nEpoch 20/25\n500/500 [==============================] - 378s 757ms/step - loss: 0.0094 - val_loss: 0.0063\nLearning rate:  1e-05\n\nEpoch 00021: LearningRateScheduler reducing learning rate to 1e-05.\nEpoch 21/25\n500/500 [==============================] - 379s 758ms/step - loss: 0.0094 - val_loss: 0.0079\nLearning rate:  1e-06\n\nEpoch 00022: LearningRateScheduler reducing learning rate to 1e-06.\nEpoch 22/25\n500/500 [==============================] - 380s 759ms/step - loss: 0.0067 - val_loss: 0.0057\nLearning rate:  1e-06\n\nEpoch 00023: LearningRateScheduler reducing learning rate to 1e-06.\nEpoch 23/25\n500/500 [==============================] - 378s 756ms/step - loss: 0.0087 - val_loss: 0.0064\nLearning rate:  1e-06\n\nEpoch 00024: LearningRateScheduler reducing learning rate to 1e-06.\nEpoch 24/25\n500/500 [==============================] - 378s 755ms/step - loss: 0.0056 - val_loss: 0.0073\nLearning rate:  1e-06\n\nEpoch 00025: LearningRateScheduler reducing learning rate to 1e-06.\nEpoch 25/25\n500/500 [==============================] - 379s 758ms/step - loss: 0.0053 - val_loss: 0.0067\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save(\"./EfficientNet_Nov8.h5\")","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'EfficientNet_Nov8.h5')","execution_count":13,"outputs":[{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"/kaggle/working/EfficientNet_Nov8.h5","text/html":"<a href='EfficientNet_Nov8.h5' target='_blank'>EfficientNet_Nov8.h5</a><br>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# Build Submission\n\nNow that the neural network is trained; we need to generate a submit CSV file to send to Kaggle.  We will use nearly the same technique to build the submit file.  However, these essential points that we must address:\n\n* We do not want the data generator to create an infinite date like we did when training.  We have a fixed number of cases to score for the Kaggle submit; we only want to process them.\n* We do not want the data generator to randomize the samples' order like it did when training. Therefore we set shuffle to false.\n* We want to always start at the beginning of the data, so we reset the generator.\n\nThese ensure that the predictions align with the id's."},{"metadata":{"trusted":true},"cell_type":"code","source":"submit_datagen = ImageDataGenerator(rescale = 1./255)\n\nsubmit_generator = submit_datagen.flow_from_dataframe(\n        dataframe=df_test,\n        directory=PATH,\n        x_col=\"filename\",\n        batch_size = 1,\n        shuffle = False,\n        target_size=(HEIGHT, WIDTH),\n        class_mode=None)\n\nsubmit_generator.reset()\npred = model.predict(submit_generator,steps=len(df_test))","execution_count":14,"outputs":[{"output_type":"stream","text":"Found 10294 validated image filenames.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_submit = pd.DataFrame({\"id\":df_test['id'],'stable':pred.flatten()})\ndf_submit.to_csv(\"./submit.csv\",index = False)","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.display import FileLink\nFileLink(r'submit.csv')","execution_count":16,"outputs":[{"output_type":"execute_result","execution_count":16,"data":{"text/plain":"/kaggle/working/submit.csv","text/html":"<a href='submit.csv' target='_blank'>submit.csv</a><br>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### Further Fine Tune with Brighter Images"},{"metadata":{"trusted":true},"cell_type":"code","source":"training_datagen = ImageDataGenerator(\n  rescale = 1./255,\n  horizontal_flip=True,\n#   featurewise_center = True,\n#   zca_epsilon = 0.001,\n#   zca_whitening = True,\n#   zoom_range = [0.7,0.8],\n  brightness_range = [0.9,1.6],\n  #Original set it to True, I will set False\n  vertical_flip = False,\n  fill_mode='nearest')\n\ntrain_generator = training_datagen.flow_from_dataframe(\n        dataframe=df_train_cut,\n        directory=PATH,\n        x_col=\"filename\",\n        y_col=\"stable\",\n        target_size=(HEIGHT, WIDTH),\n        batch_size=16,\n        class_mode='binary')\n\nvalidation_datagen = ImageDataGenerator(rescale = 1./255,\n#                                           featurewise_center = True,\n#                                           zca_epsilon = 0.001,\n#                                           zca_whitening = True,\n#                                           zoom_range = [0.7,0.8],\n                                          brightness_range = [1.2,1.5])\n\nval_generator = validation_datagen.flow_from_dataframe(\n        dataframe=df_validate_cut,\n        directory=PATH,\n        x_col=\"filename\",\n        y_col=\"stable\",\n        target_size=(HEIGHT, WIDTH),\n        class_mode='binary')\n\nfrom keras.models import load_model\nmodel_tune_2 = load_model(\"Xception_Tune_Oct_7.h5\")\n\ndef lr_schedule2(epoch):\n    lr = 0.00005\n    if epoch > 5:\n        lr = 0.00001\n    return lr\n\nlr_callback2 = tf.keras.callbacks.LearningRateScheduler(lr_schedule2, verbose=True)\nmonitor2 = EarlyStopping(monitor='val_loss', min_delta=1e-4, patience=10, verbose=1, mode='auto',\n        restore_best_weights=True)\ncallback_list2 = [monitor2, lr_callback2]\n\n\nhistory = model_tune.fit(train_generator,  \n  verbose = 1, \n  validation_data=val_generator, \n  steps_per_epoch=500, \n  validation_steps=100,\n  epochs=10,callbacks=callback_list2)","execution_count":17,"outputs":[{"output_type":"stream","text":"Found 32785 validated image filenames belonging to 2 classes.\nFound 8196 validated image filenames belonging to 2 classes.\n","name":"stdout"},{"output_type":"error","ename":"OSError","evalue":"SavedModel file does not exist at: Xception_Tune_Oct_7.h5/{saved_model.pbtxt|saved_model.pb}","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-9f6d8c601865>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mmodel_tune_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Xception_Tune_Oct_7.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlr_schedule2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath_to_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m       \u001b[0mloader_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0msaved_model_load\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/saved_model/loader_impl.py\u001b[0m in \u001b[0;36mparse_saved_model\u001b[0;34m(export_dir)\u001b[0m\n\u001b[1;32m    111\u001b[0m                   (export_dir,\n\u001b[1;32m    112\u001b[0m                    \u001b[0mconstants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSAVED_MODEL_FILENAME_PBTXT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                    constants.SAVED_MODEL_FILENAME_PB))\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: SavedModel file does not exist at: Xception_Tune_Oct_7.h5/{saved_model.pbtxt|saved_model.pb}"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_tune.save(\"./ResNet101_Nov8.h5\")\nfrom IPython.display import FileLink\nFileLink(r'ResNet101_Nov8.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}
